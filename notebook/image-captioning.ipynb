{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:08:36.971664Z",
     "iopub.status.busy": "2025-10-02T17:08:36.971443Z",
     "iopub.status.idle": "2025-10-02T17:09:03.378312Z",
     "shell.execute_reply": "2025-10-02T17:09:03.377749Z",
     "shell.execute_reply.started": "2025-10-02T17:08:36.971635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 17:08:52.409293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759424932.607631      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759424932.663477      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy \n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet50,ResNet50_Weights\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:03.380015Z",
     "iopub.status.busy": "2025-10-02T17:09:03.379542Z",
     "iopub.status.idle": "2025-10-02T17:09:03.485770Z",
     "shell.execute_reply": "2025-10-02T17:09:03.484942Z",
     "shell.execute_reply.started": "2025-10-02T17:09:03.379997Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40450</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man in a pink shirt climbs a rock face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40451</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man is rock climbing high in the air .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40452</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40453</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber in a red shirt .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40454</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber practices on a rock climbing wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40455 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0      1000268201_693b08cb0e.jpg   \n",
       "1      1000268201_693b08cb0e.jpg   \n",
       "2      1000268201_693b08cb0e.jpg   \n",
       "3      1000268201_693b08cb0e.jpg   \n",
       "4      1000268201_693b08cb0e.jpg   \n",
       "...                          ...   \n",
       "40450   997722733_0cb5439472.jpg   \n",
       "40451   997722733_0cb5439472.jpg   \n",
       "40452   997722733_0cb5439472.jpg   \n",
       "40453   997722733_0cb5439472.jpg   \n",
       "40454   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption  \n",
       "0      A child in a pink dress is climbing up a set o...  \n",
       "1                  A girl going into a wooden building .  \n",
       "2       A little girl climbing into a wooden playhouse .  \n",
       "3      A little girl climbing the stairs to her playh...  \n",
       "4      A little girl in a pink dress going into a woo...  \n",
       "...                                                  ...  \n",
       "40450           A man in a pink shirt climbs a rock face  \n",
       "40451           A man is rock climbing high in the air .  \n",
       "40452  A person in a red shirt climbing up a rock fac...  \n",
       "40453                    A rock climber in a red shirt .  \n",
       "40454  A rock climber practices on a rock climbing wa...  \n",
       "\n",
       "[40455 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/flickr8k/captions.txt')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Main task: to convert text to numerical values\n",
    "\n",
    "1. vocab mapping each word to an index\n",
    "2. setup pytorch dataset to load the data\n",
    "3. setup padding of every batch (so all the sequence_length are same and then setup dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:03.486696Z",
     "iopub.status.busy": "2025-10-02T17:09:03.486472Z",
     "iopub.status.idle": "2025-10-02T17:09:04.174136Z",
     "shell.execute_reply": "2025-10-02T17:09:04.173530Z",
     "shell.execute_reply.started": "2025-10-02T17:09:03.486678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'sentence', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "spacy_eng= spacy.load(\"en_core_web_sm\")\n",
    "#testing\n",
    "[tok.text.lower() for tok in spacy_eng.tokenizer('This is a sentence.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:04.175091Z",
     "iopub.status.busy": "2025-10-02T17:09:04.174819Z",
     "iopub.status.idle": "2025-10-02T17:09:04.181764Z",
     "shell.execute_reply": "2025-10-02T17:09:04.181192Z",
     "shell.execute_reply.started": "2025-10-02T17:09:04.175036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self,freq_threshold):\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\":0,\"<SOS>\":1,\"<EOS>\":2,\"<UNK>\":3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod #this assures the function is under vocabulary but doesnot attend to its objects state.\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self,sentence_list):\n",
    "        '''\n",
    "        sentence_list: input to the function of list of sentences\n",
    "\n",
    "        Flow: Loop over sentencelist -> for each word in sentence update its freq\n",
    "              -> if freq reaches threshold then add it to the vocabulary, else not(<UNK>).\n",
    "\n",
    "        Note: The counter starts from 4, because 0-3 are reserved.\n",
    "        '''\n",
    "        frequencies = {}\n",
    "        idx = 4 # 0-3 are reserved for pad,sos,bos,unk\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                frequencies[word] = frequencies.get(word,0) + 1 # new word gets 1, if word already exists then updates the counter\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self,text):\n",
    "        '''\n",
    "        Takes in text and returns the id of the tokenized text if in stoi else <UNK>\n",
    "        '''\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:04.182752Z",
     "iopub.status.busy": "2025-10-02T17:09:04.182508Z",
     "iopub.status.idle": "2025-10-02T17:09:04.199128Z",
     "shell.execute_reply": "2025-10-02T17:09:04.198607Z",
     "shell.execute_reply.started": "2025-10-02T17:09:04.182730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self,root_dir,captions_file,transform=None,freq_threshold=5):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        # get image and caption columns\n",
    "        self.imgs = self.df[\"image\"]\n",
    "        self.captions = self.df[\"caption\"]\n",
    "        \n",
    "        #initialize vocabulary and build vocab\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        '''\n",
    "        Takes in particular index, maps to the index-th row of dataframe with \n",
    "        image,caption pair. Then wraps the numericalized caption with SOS and EOS.\n",
    "        Returns the tensor of the final list of ids.\n",
    "        '''\n",
    "        #pull a row(image,caption pair) from dataframe\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        # load the image with PIL\n",
    "        img = Image.open(os.path.join(self.root_dir,img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # wrapping up the numericalized token with SOS and EOS at first and last respectively.\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:04.200312Z",
     "iopub.status.busy": "2025-10-02T17:09:04.200033Z",
     "iopub.status.idle": "2025-10-02T17:09:04.212642Z",
     "shell.execute_reply": "2025-10-02T17:09:04.212085Z",
     "shell.execute_reply.started": "2025-10-02T17:09:04.200291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    '''\n",
    "    This class aims to match the sequence of every text sequence using <PAD> tokens.\n",
    "    For Images, This class introduces a Batch dimension making it [B,C,H,W] from [C,H,W]\n",
    "    '''\n",
    "    def __init__(self,pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self,batch):\n",
    "        '''\n",
    "        Input Example: batch -> [(img1_tensor,[1,4,8,4]),.... ], The image tensor size remains same, but \n",
    "        text tensor sequence length may differ so to fix we add pad tokens.\n",
    "\n",
    "        For images: Take the each 1st value of each item from batch and unsqueeze at dimension 0 to add batch dim in all the image tensor\n",
    "        then concatenate along the 0th(batch) dimension.\n",
    "\n",
    "        For captions (as targets): Take the 2nd value of each item from batch and then pad it with pad value\n",
    "        '''\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs,dim=0)\n",
    "\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets,batch_first = False,padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs,targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate DataLoaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:04.215245Z",
     "iopub.status.busy": "2025-10-02T17:09:04.214792Z",
     "iopub.status.idle": "2025-10-02T17:09:04.226131Z",
     "shell.execute_reply": "2025-10-02T17:09:04.225513Z",
     "shell.execute_reply.started": "2025-10-02T17:09:04.215224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "    root_folder,\n",
    "    annotation_file,\n",
    "    transform,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    split_ratio=0.8\n",
    "):\n",
    "    dataset = FlickrDataset(root_folder,annotation_file,transform=transform)\n",
    "    #train val split\n",
    "    train_size = int(split_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset,[train_size,val_size])\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "        drop_last = False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "        drop_last = False\n",
    "    )\n",
    "    return train_loader,val_loader,dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "* Useful for pausing during training using 'checkpoints'.\n",
    "1. save_checkpoint\n",
    "2. load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:04.227151Z",
     "iopub.status.busy": "2025-10-02T17:09:04.226919Z",
     "iopub.status.idle": "2025-10-02T17:09:04.249298Z",
     "shell.execute_reply": "2025-10-02T17:09:04.248703Z",
     "shell.execute_reply.started": "2025-10-02T17:09:04.227135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state,filename='my_checkpoint.pth.tar'):\n",
    "    print(\"Saving Checkpoint\")\n",
    "    torch.save(state,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:04.250052Z",
     "iopub.status.busy": "2025-10-02T17:09:04.249862Z",
     "iopub.status.idle": "2025-10-02T17:09:04.259266Z",
     "shell.execute_reply": "2025-10-02T17:09:04.258589Z",
     "shell.execute_reply.started": "2025-10-02T17:09:04.250038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "1. CNN as an encoder (ResNet50) for images to capture those features\n",
    "2. RNN as a decoder (LSTM) for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:04.260209Z",
     "iopub.status.busy": "2025-10-02T17:09:04.259894Z",
     "iopub.status.idle": "2025-10-02T17:09:04.269853Z",
     "shell.execute_reply": "2025-10-02T17:09:04.269218Z",
     "shell.execute_reply.started": "2025-10-02T17:09:04.260167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self,embed_size,train_CNN=False):\n",
    "        super(EncoderCNN,self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features,embed_size) #removing the classifier layer, then preserving the features in embed_size size.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        for name,param in self.resnet.named_parameters():\n",
    "            param.requires_grad = (train_CNN or 'fc' in name)\n",
    "\n",
    "    def forward(self,images):\n",
    "        features = self.resnet(images)\n",
    "        return self.dropout(self.relu(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:04.270640Z",
     "iopub.status.busy": "2025-10-02T17:09:04.270429Z",
     "iopub.status.idle": "2025-10-02T17:09:04.280936Z",
     "shell.execute_reply": "2025-10-02T17:09:04.280249Z",
     "shell.execute_reply.started": "2025-10-02T17:09:04.270625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers)\n",
    "        self.linear = nn.Linear(hidden_size,vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,features,captions):\n",
    "        '''\n",
    "        Adds a sequence dimension to the features (dim0)[1,batch_size,embed_size] and then concatenates along dim0.\n",
    "        First timestep gives image feature, Next timestep gives caption embeddings.\n",
    "        Final shape becomes:[1+seq_len,batch_size,embed_size]\n",
    "        '''\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        embeddings = torch.cat((features.unsqueeze(0),embeddings),dim=0)\n",
    "        hiddens,_ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:04.281868Z",
     "iopub.status.busy": "2025-10-02T17:09:04.281677Z",
     "iopub.status.idle": "2025-10-02T17:09:04.292717Z",
     "shell.execute_reply": "2025-10-02T17:09:04.292110Z",
     "shell.execute_reply.started": "2025-10-02T17:09:04.281853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers):\n",
    "        super(CNNtoRNN,self).__init__()\n",
    "        self.CNN = EncoderCNN(embed_size)\n",
    "        self.RNN = DecoderRNN(embed_size,hidden_size,vocab_size,num_layers)\n",
    "\n",
    "    def forward(self,images,captions):\n",
    "        features = self.CNN(images)\n",
    "        outputs = self.RNN(features,captions)\n",
    "        return outputs\n",
    "        \n",
    "    def caption_image(self,image,vocabulary,max_len=50):\n",
    "        '''\n",
    "        During inference or evaluation we wont have target captions(which is going to be predicted,duh!).\n",
    "        image: inference image\n",
    "        vocabulary: whole mapping dictionary\n",
    "        max_len: max length for captions to be predicted.\n",
    "        '''\n",
    "        caption_result = []\n",
    "        with torch.no_grad():\n",
    "            x = self.CNN(image).unsqueeze(0) # add batch dimension at dim0\n",
    "            states = None\n",
    "            for _ in range(max_len):\n",
    "                hiddens,states = self.RNN.lstm(x,states)\n",
    "                output = self.RNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                \n",
    "                caption_result.append(predicted.item())\n",
    "                x = self.RNN.embed(predicted).unsqueeze(0)\n",
    "                if vocabulary.itos[predicted.item()] == '<EOS>':\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in caption_result]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:04.293644Z",
     "iopub.status.busy": "2025-10-02T17:09:04.293413Z",
     "iopub.status.idle": "2025-10-02T17:09:05.302911Z",
     "shell.execute_reply": "2025-10-02T17:09:05.302134Z",
     "shell.execute_reply.started": "2025-10-02T17:09:04.293625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(356),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #from pytorch docs(resnet)\n",
    "    ]\n",
    "    )\n",
    "\n",
    "train_loader ,val_loader, dataset = get_loader(\n",
    "    root_folder=\"/kaggle/input/flickr8k/Images/\",\n",
    "    annotation_file=\"/kaggle/input/flickr8k/captions.txt\",\n",
    "    transform = transform,\n",
    "    num_workers=2\n",
    "    )\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "load_model = False\n",
    "save_model = True\n",
    "train_CNN = False #set this true to train all the params of the ResNet model\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "vocab_size = len(dataset.vocab)\n",
    "num_layers = 1\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:09:05.303963Z",
     "iopub.status.busy": "2025-10-02T17:09:05.303703Z",
     "iopub.status.idle": "2025-10-02T17:33:53.285150Z",
     "shell.execute_reply": "2025-10-02T17:33:53.284250Z",
     "shell.execute_reply.started": "2025-10-02T17:09:05.303940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 212MB/s]\n",
      "Epoch [1/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:50<00:00,  5.92batch/s, avg_loss=3.8, batch_loss=3.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Checkpoint\n",
      "Epoch 1 complete. Avg Train Loss: 3.7959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:25<00:00,  6.95batch/s, avg_loss=3.11, batch_loss=3.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Checkpoint\n",
      "Epoch 2 complete. Avg Train Loss: 3.1116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:24<00:00,  6.98batch/s, avg_loss=2.89, batch_loss=2.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Checkpoint\n",
      "Epoch 3 complete. Avg Train Loss: 2.8881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:26<00:00,  6.93batch/s, avg_loss=2.74, batch_loss=3.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Checkpoint\n",
      "Epoch 4 complete. Avg Train Loss: 2.7403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:25<00:00,  6.94batch/s, avg_loss=2.63, batch_loss=2.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Checkpoint\n",
      "Epoch 5 complete. Avg Train Loss: 2.6305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:25<00:00,  6.95batch/s, avg_loss=2.54, batch_loss=2.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Checkpoint\n",
      "Epoch 6 complete. Avg Train Loss: 2.5448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:25<00:00,  6.94batch/s, avg_loss=2.48, batch_loss=2.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Checkpoint\n",
      "Epoch 7 complete. Avg Train Loss: 2.4756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:28<00:00,  6.82batch/s, avg_loss=2.42, batch_loss=2.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Checkpoint\n",
      "Epoch 8 complete. Avg Train Loss: 2.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:25<00:00,  6.96batch/s, avg_loss=2.37, batch_loss=2.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Checkpoint\n",
      "Epoch 9 complete. Avg Train Loss: 2.3658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:24<00:00,  7.00batch/s, avg_loss=2.32, batch_loss=2.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Checkpoint\n",
      "Epoch 10 complete. Avg Train Loss: 2.3212\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    torch.backends.cudnn.benchmark = True #10-20% faster training on gpu\n",
    "\n",
    "    writer = SummaryWriter(\"runs/flickr\") #logging details from training for review\n",
    "\n",
    "    step = 0 # training step counter (based on epochs)\n",
    "\n",
    "    model = CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "    optimizer = optim.Adam((p for p in model.parameters() if p.requires_grad),lr=learning_rate) # just to avoid wasting memory on frozen params.\n",
    "\n",
    "    # Can continue the training from checkpoint if saved.\n",
    "    if load_model:\n",
    "        checkpoint_path=\"my_checkpoint.pth.tar\"\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            try:\n",
    "                step = load_checkpoint(torch.load(checkpoint_path),model,optimizer)\n",
    "                print(\"Checkpoint Loaded.\")\n",
    "            except Exception as e:\n",
    "                print(\"Error loading checkpoint:{e}\")\n",
    "                print(\"Training from scratch...\")\n",
    "                step=0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "        loop = tqdm(\n",
    "            enumerate(train_loader),\n",
    "            total=len(train_loader),\n",
    "            desc=f\"Epoch [{epoch+1}/{num_epochs}]\",\n",
    "            unit=\"batch\",\n",
    "            leave=True\n",
    "        )\n",
    "        for batch_idx, (imgs, captions) in loop:\n",
    "            imgs, captions = imgs.to(device), captions.to(device)\n",
    "            outputs = model(imgs, captions[:-1])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            loop.set_postfix(batch_loss=loss.item(),\n",
    "            avg_loss=total_train_loss/(batch_idx+1))\n",
    "    \n",
    "            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step += 1\n",
    "        if save_model:\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"step\": step\n",
    "                        }\n",
    "            save_checkpoint(checkpoint)\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} complete. Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "    torch.save(model.state_dict(),\"final_model.pth\")\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:37:21.904877Z",
     "iopub.status.busy": "2025-10-02T17:37:21.904242Z",
     "iopub.status.idle": "2025-10-02T17:39:12.979186Z",
     "shell.execute_reply": "2025-10-02T17:39:12.978469Z",
     "shell.execute_reply.started": "2025-10-02T17:37:21.904846Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-02 17:37:23.043647: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759426643.064453     237 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759426643.071423     237 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs #for training details after completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:39:17.789009Z",
     "iopub.status.busy": "2025-10-02T17:39:17.788714Z",
     "iopub.status.idle": "2025-10-02T17:39:55.865233Z",
     "shell.execute_reply": "2025-10-02T17:39:55.864413Z",
     "shell.execute_reply.started": "2025-10-02T17:39:17.788984Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [00:37<00:00,  6.77batch/s, avg_loss=2.45, batch_loss=2.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Avg Validation Loss: 2.4506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def eval():\n",
    "    model = CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers).to(device)\n",
    "    model.load_state_dict(torch.load(\"final_model.pth\"))\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    loop = tqdm(\n",
    "        enumerate(val_loader),\n",
    "        total=len(val_loader),\n",
    "        desc=f\"Evaluation\",\n",
    "        unit=\"batch\",\n",
    "        leave=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (imgs, captions) in loop:\n",
    "            imgs, captions = imgs.to(device), captions.to(device)\n",
    "            outputs = model(imgs, captions[:-1])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "            loop.set_postfix(batch_loss=loss.item(),\n",
    "            avg_loss=total_val_loss/(batch_idx+1))\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Evaluation complete. Avg Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T17:44:53.337716Z",
     "iopub.status.busy": "2025-10-02T17:44:53.336899Z",
     "iopub.status.idle": "2025-10-02T17:44:55.222320Z",
     "shell.execute_reply": "2025-10-02T17:44:55.221325Z",
     "shell.execute_reply.started": "2025-10-02T17:44:53.337686Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted: <SOS> a white dog is running through the grass . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def inference():\n",
    "    inference_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(356),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    _,val_loader,dataset = get_loader(\n",
    "        root_folder=\"/kaggle/input/flickr8k/Images/\",\n",
    "        annotation_file=\"/kaggle/input/flickr8k/captions.txt\",\n",
    "        transform = inference_transform,\n",
    "        num_workers=2)\n",
    "    \n",
    "    model = CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers).to(device)\n",
    "    model.load_state_dict(torch.load(\"final_model.pth\"))\n",
    "    model.eval()\n",
    "    imgs, captions = next(iter(val_loader))\n",
    "    imgs, captions = imgs.to(device), captions.to(device)\n",
    "    y_pred = model.caption_image(imgs[3].unsqueeze(0), dataset.vocab)  # one image\n",
    "    print(\"\\nPredicted:\", \" \".join(y_pred))\n",
    "\n",
    "\n",
    "inference()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
