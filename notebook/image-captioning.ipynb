{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import dependencies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport spacy \nimport os\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom torchvision.models import resnet50,ResNet50_Weights\nfrom torch.utils.data import DataLoader, Dataset, random_split","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:08:36.971443Z","iopub.execute_input":"2025-10-02T17:08:36.971664Z","iopub.status.idle":"2025-10-02T17:09:03.378312Z","shell.execute_reply.started":"2025-10-02T17:08:36.971635Z","shell.execute_reply":"2025-10-02T17:09:03.377749Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-10-02 17:08:52.409293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759424932.607631      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759424932.663477      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/flickr8k/captions.txt')\ndf","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:03.379542Z","iopub.execute_input":"2025-10-02T17:09:03.380015Z","iopub.status.idle":"2025-10-02T17:09:03.485770Z","shell.execute_reply.started":"2025-10-02T17:09:03.379997Z","shell.execute_reply":"2025-10-02T17:09:03.484942Z"},"trusted":true},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                           image  \\\n0      1000268201_693b08cb0e.jpg   \n1      1000268201_693b08cb0e.jpg   \n2      1000268201_693b08cb0e.jpg   \n3      1000268201_693b08cb0e.jpg   \n4      1000268201_693b08cb0e.jpg   \n...                          ...   \n40450   997722733_0cb5439472.jpg   \n40451   997722733_0cb5439472.jpg   \n40452   997722733_0cb5439472.jpg   \n40453   997722733_0cb5439472.jpg   \n40454   997722733_0cb5439472.jpg   \n\n                                                 caption  \n0      A child in a pink dress is climbing up a set o...  \n1                  A girl going into a wooden building .  \n2       A little girl climbing into a wooden playhouse .  \n3      A little girl climbing the stairs to her playh...  \n4      A little girl in a pink dress going into a woo...  \n...                                                  ...  \n40450           A man in a pink shirt climbs a rock face  \n40451           A man is rock climbing high in the air .  \n40452  A person in a red shirt climbing up a rock fac...  \n40453                    A rock climber in a red shirt .  \n40454  A rock climber practices on a rock climbing wa...  \n\n[40455 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing the stairs to her playh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl in a pink dress going into a woo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>40450</th>\n      <td>997722733_0cb5439472.jpg</td>\n      <td>A man in a pink shirt climbs a rock face</td>\n    </tr>\n    <tr>\n      <th>40451</th>\n      <td>997722733_0cb5439472.jpg</td>\n      <td>A man is rock climbing high in the air .</td>\n    </tr>\n    <tr>\n      <th>40452</th>\n      <td>997722733_0cb5439472.jpg</td>\n      <td>A person in a red shirt climbing up a rock fac...</td>\n    </tr>\n    <tr>\n      <th>40453</th>\n      <td>997722733_0cb5439472.jpg</td>\n      <td>A rock climber in a red shirt .</td>\n    </tr>\n    <tr>\n      <th>40454</th>\n      <td>997722733_0cb5439472.jpg</td>\n      <td>A rock climber practices on a rock climbing wa...</td>\n    </tr>\n  </tbody>\n</table>\n<p>40455 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Preprocess Data","metadata":{}},{"cell_type":"markdown","source":"\nMain task: to convert text to numerical values\n\n1. vocab mapping each word to an index\n2. setup pytorch dataset to load the data\n3. setup padding of every batch (so all the sequence_length are same and then setup dataloader)","metadata":{}},{"cell_type":"code","source":"# !python -m spacy download en_core_web_sm\nspacy_eng= spacy.load(\"en_core_web_sm\")\n#testing\n[tok.text.lower() for tok in spacy_eng.tokenizer('This is a sentence.')]","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:03.486472Z","iopub.execute_input":"2025-10-02T17:09:03.486696Z","iopub.status.idle":"2025-10-02T17:09:04.174136Z","shell.execute_reply.started":"2025-10-02T17:09:03.486678Z","shell.execute_reply":"2025-10-02T17:09:04.173530Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['this', 'is', 'a', 'sentence', '.']"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### Vocabulary Mapping","metadata":{}},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self,freq_threshold):\n        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n        self.stoi = {\"<PAD>\":0,\"<SOS>\":1,\"<EOS>\":2,\"<UNK>\":3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod #this assures the function is under vocabulary but doesnot attend to its objects state.\n    def tokenizer_eng(text):\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n\n    def build_vocabulary(self,sentence_list):\n        '''\n        sentence_list: input to the function of list of sentences\n\n        Flow: Loop over sentencelist -> for each word in sentence update its freq\n              -> if freq reaches threshold then add it to the vocabulary, else not(<UNK>).\n\n        Note: The counter starts from 4, because 0-3 are reserved.\n        '''\n        frequencies = {}\n        idx = 4 # 0-3 are reserved for pad,sos,bos,unk\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                frequencies[word] = frequencies.get(word,0) + 1 # new word gets 1, if word already exists then updates the counter\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self,text):\n        '''\n        Takes in text and returns the id of the tokenized text if in stoi else <UNK>\n        '''\n        tokenized_text = self.tokenizer_eng(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]\n\n        ","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:04.174819Z","iopub.execute_input":"2025-10-02T17:09:04.175091Z","iopub.status.idle":"2025-10-02T17:09:04.181764Z","shell.execute_reply.started":"2025-10-02T17:09:04.175036Z","shell.execute_reply":"2025-10-02T17:09:04.181192Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Dataset Definition","metadata":{}},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self,root_dir,captions_file,transform=None,freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(captions_file)\n        self.transform = transform\n\n        # get image and caption columns\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n        \n        #initialize vocabulary and build vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.captions.tolist())\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self,index):\n        '''\n        Takes in particular index, maps to the index-th row of dataframe with \n        image,caption pair. Then wraps the numericalized caption with SOS and EOS.\n        Returns the tensor of the final list of ids.\n        '''\n        #pull a row(image,caption pair) from dataframe\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        # load the image with PIL\n        img = Image.open(os.path.join(self.root_dir,img_id)).convert(\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        # wrapping up the numericalized token with SOS and EOS at first and last respectively.\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return img, torch.tensor(numericalized_caption)\n    ","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:04.182508Z","iopub.execute_input":"2025-10-02T17:09:04.182752Z","iopub.status.idle":"2025-10-02T17:09:04.199128Z","shell.execute_reply.started":"2025-10-02T17:09:04.182730Z","shell.execute_reply":"2025-10-02T17:09:04.198607Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Collate Functionality","metadata":{}},{"cell_type":"code","source":"class MyCollate:\n    '''\n    This class aims to match the sequence of every text sequence using <PAD> tokens.\n    For Images, This class introduces a Batch dimension making it [B,C,H,W] from [C,H,W]\n    '''\n    def __init__(self,pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self,batch):\n        '''\n        Input Example: batch -> [(img1_tensor,[1,4,8,4]),.... ], The image tensor size remains same, but \n        text tensor sequence length may differ so to fix we add pad tokens.\n\n        For images: Take the each 1st value of each item from batch and unsqueeze at dimension 0 to add batch dim in all the image tensor\n        then concatenate along the 0th(batch) dimension.\n\n        For captions (as targets): Take the 2nd value of each item from batch and then pad it with pad value\n        '''\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs,dim=0)\n\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets,batch_first = False,padding_value=self.pad_idx)\n\n        return imgs,targets","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:04.200033Z","iopub.execute_input":"2025-10-02T17:09:04.200312Z","iopub.status.idle":"2025-10-02T17:09:04.212642Z","shell.execute_reply.started":"2025-10-02T17:09:04.200291Z","shell.execute_reply":"2025-10-02T17:09:04.212085Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Generate DataLoaders ","metadata":{}},{"cell_type":"code","source":"def get_loader(\n    root_folder,\n    annotation_file,\n    transform,\n    batch_size=32,\n    num_workers=8,\n    shuffle=True,\n    pin_memory=True,\n    split_ratio=0.8\n):\n    dataset = FlickrDataset(root_folder,annotation_file,transform=transform)\n    #train val split\n    train_size = int(split_ratio * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset,[train_size,val_size])\n\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n    \n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=pin_memory,\n        collate_fn=MyCollate(pad_idx=pad_idx),\n        drop_last = False\n    )\n    val_loader = DataLoader(\n        dataset=val_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n        pin_memory=pin_memory,\n        collate_fn=MyCollate(pad_idx=pad_idx),\n        drop_last = False\n    )\n    return train_loader,val_loader,dataset","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:04.214792Z","iopub.execute_input":"2025-10-02T17:09:04.215245Z","iopub.status.idle":"2025-10-02T17:09:04.226131Z","shell.execute_reply.started":"2025-10-02T17:09:04.215224Z","shell.execute_reply":"2025-10-02T17:09:04.225513Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Utility functions\n* Useful for pausing during training using 'checkpoints'.\n1. save_checkpoint\n2. load_checkpoint","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(state,filename='my_checkpoint.pth.tar'):\n    print(\"Saving Checkpoint\")\n    torch.save(state,filename)","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:04.226919Z","iopub.execute_input":"2025-10-02T17:09:04.227151Z","iopub.status.idle":"2025-10-02T17:09:04.249298Z","shell.execute_reply.started":"2025-10-02T17:09:04.227135Z","shell.execute_reply":"2025-10-02T17:09:04.248703Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    step = checkpoint[\"step\"]\n    return step","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:04.249862Z","iopub.execute_input":"2025-10-02T17:09:04.250052Z","iopub.status.idle":"2025-10-02T17:09:04.259266Z","shell.execute_reply.started":"2025-10-02T17:09:04.250038Z","shell.execute_reply":"2025-10-02T17:09:04.258589Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Model Architecture\n\n1. CNN as an encoder (ResNet50) for images to capture those features\n2. RNN as a decoder (LSTM) for text generation","metadata":{}},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self,embed_size,train_CNN=False):\n        super(EncoderCNN,self).__init__()\n        self.train_CNN = train_CNN\n        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features,embed_size) #removing the classifier layer, then preserving the features in embed_size size.\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n        for name,param in self.resnet.named_parameters():\n            param.requires_grad = (train_CNN or 'fc' in name)\n\n    def forward(self,images):\n        features = self.resnet(images)\n        return self.dropout(self.relu(features))\n","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:04.259894Z","iopub.execute_input":"2025-10-02T17:09:04.260209Z","iopub.status.idle":"2025-10-02T17:09:04.269853Z","shell.execute_reply.started":"2025-10-02T17:09:04.260167Z","shell.execute_reply":"2025-10-02T17:09:04.269218Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self,embed_size,hidden_size,vocab_size,num_layers):\n        super(DecoderRNN,self).__init__()\n        self.embed = nn.Embedding(vocab_size,embed_size)\n        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers)\n        self.linear = nn.Linear(hidden_size,vocab_size)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self,features,captions):\n        '''\n        Adds a sequence dimension to the features (dim0)[1,batch_size,embed_size] and then concatenates along dim0.\n        First timestep gives image feature, Next timestep gives caption embeddings.\n        Final shape becomes:[1+seq_len,batch_size,embed_size]\n        '''\n        embeddings = self.dropout(self.embed(captions))\n        embeddings = torch.cat((features.unsqueeze(0),embeddings),dim=0)\n        hiddens,_ = self.lstm(embeddings)\n        outputs = self.linear(hiddens)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:04.270429Z","iopub.execute_input":"2025-10-02T17:09:04.270640Z","iopub.status.idle":"2025-10-02T17:09:04.280936Z","shell.execute_reply.started":"2025-10-02T17:09:04.270625Z","shell.execute_reply":"2025-10-02T17:09:04.280249Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class CNNtoRNN(nn.Module):\n    def __init__(self,embed_size,hidden_size,vocab_size,num_layers):\n        super(CNNtoRNN,self).__init__()\n        self.CNN = EncoderCNN(embed_size)\n        self.RNN = DecoderRNN(embed_size,hidden_size,vocab_size,num_layers)\n\n    def forward(self,images,captions):\n        features = self.CNN(images)\n        outputs = self.RNN(features,captions)\n        return outputs\n        \n    def caption_image(self,image,vocabulary,max_len=50):\n        '''\n        During inference or evaluation we wont have target captions(which is going to be predicted,duh!).\n        image: inference image\n        vocabulary: whole mapping dictionary\n        max_len: max length for captions to be predicted.\n        '''\n        caption_result = []\n        with torch.no_grad():\n            x = self.CNN(image).unsqueeze(0) # add batch dimension at dim0\n            states = None\n            for _ in range(max_len):\n                hiddens,states = self.RNN.lstm(x,states)\n                output = self.RNN.linear(hiddens.squeeze(0))\n                predicted = output.argmax(1)\n                \n                caption_result.append(predicted.item())\n                x = self.RNN.embed(predicted).unsqueeze(0)\n                if vocabulary.itos[predicted.item()] == '<EOS>':\n                    break\n\n        return [vocabulary.itos[idx] for idx in caption_result]\n                ","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:04.281677Z","iopub.execute_input":"2025-10-02T17:09:04.281868Z","iopub.status.idle":"2025-10-02T17:09:04.292717Z","shell.execute_reply.started":"2025-10-02T17:09:04.281853Z","shell.execute_reply":"2025-10-02T17:09:04.292110Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Train Model","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose(\n    [\n        transforms.Resize(356),\n        transforms.RandomCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #from pytorch docs(resnet)\n    ]\n    )\n\ntrain_loader ,val_loader, dataset = get_loader(\n    root_folder=\"/kaggle/input/flickr8k/Images/\",\n    annotation_file=\"/kaggle/input/flickr8k/captions.txt\",\n    transform = transform,\n    num_workers=2\n    )\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nload_model = False\nsave_model = True\ntrain_CNN = False #set this true to train all the params of the ResNet model\n\nembed_size = 256\nhidden_size = 256\nvocab_size = len(dataset.vocab)\nnum_layers = 1\nlearning_rate = 3e-4\nnum_epochs = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T17:09:04.293413Z","iopub.execute_input":"2025-10-02T17:09:04.293644Z","iopub.status.idle":"2025-10-02T17:09:05.302911Z","shell.execute_reply.started":"2025-10-02T17:09:04.293625Z","shell.execute_reply":"2025-10-02T17:09:05.302134Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train():\n    torch.backends.cudnn.benchmark = True #10-20% faster training on gpu\n\n    writer = SummaryWriter(\"runs/flickr\") #logging details from training for review\n\n    step = 0 # training step counter (based on epochs)\n\n    model = CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers).to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n    optimizer = optim.Adam((p for p in model.parameters() if p.requires_grad),lr=learning_rate) # just to avoid wasting memory on frozen params.\n\n    # Can continue the training from checkpoint if saved.\n    if load_model:\n        checkpoint_path=\"my_checkpoint.pth.tar\"\n        if os.path.exists(checkpoint_path):\n            try:\n                step = load_checkpoint(torch.load(checkpoint_path),model,optimizer)\n                print(\"Checkpoint Loaded.\")\n            except Exception as e:\n                print(\"Error loading checkpoint:{e}\")\n                print(\"Training from scratch...\")\n                step=0\n\n    model.train()\n    for epoch in range(num_epochs):\n        total_train_loss = 0\n        loop = tqdm(\n            enumerate(train_loader),\n            total=len(train_loader),\n            desc=f\"Epoch [{epoch+1}/{num_epochs}]\",\n            unit=\"batch\",\n            leave=True\n        )\n        for batch_idx, (imgs, captions) in loop:\n            imgs, captions = imgs.to(device), captions.to(device)\n            outputs = model(imgs, captions[:-1])\n            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n    \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item()\n            loop.set_postfix(batch_loss=loss.item(),\n            avg_loss=total_train_loss/(batch_idx+1))\n    \n            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n            step += 1\n        if save_model:\n            checkpoint = {\n                \"state_dict\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"step\": step\n                        }\n            save_checkpoint(checkpoint)\n        avg_train_loss = total_train_loss / len(train_loader)\n        print(f\"Epoch {epoch+1} complete. Avg Train Loss: {avg_train_loss:.4f}\")\n    torch.save(model.state_dict(),\"final_model.pth\")\n    print(\"Model saved successfully!\")\n\ntrain()","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:09:05.303703Z","iopub.execute_input":"2025-10-02T17:09:05.303963Z","iopub.status.idle":"2025-10-02T17:33:53.285150Z","shell.execute_reply.started":"2025-10-02T17:09:05.303940Z","shell.execute_reply":"2025-10-02T17:33:53.284250Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 212MB/s]\nEpoch [1/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:50<00:00,  5.92batch/s, avg_loss=3.8, batch_loss=3.46]\n","output_type":"stream"},{"name":"stdout","text":"Saving Checkpoint\nEpoch 1 complete. Avg Train Loss: 3.7959\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:25<00:00,  6.95batch/s, avg_loss=3.11, batch_loss=3.08]\n","output_type":"stream"},{"name":"stdout","text":"Saving Checkpoint\nEpoch 2 complete. Avg Train Loss: 3.1116\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:24<00:00,  6.98batch/s, avg_loss=2.89, batch_loss=2.99]\n","output_type":"stream"},{"name":"stdout","text":"Saving Checkpoint\nEpoch 3 complete. Avg Train Loss: 2.8881\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:26<00:00,  6.93batch/s, avg_loss=2.74, batch_loss=3.2] \n","output_type":"stream"},{"name":"stdout","text":"Saving Checkpoint\nEpoch 4 complete. Avg Train Loss: 2.7403\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:25<00:00,  6.94batch/s, avg_loss=2.63, batch_loss=2.47]\n","output_type":"stream"},{"name":"stdout","text":"Saving Checkpoint\nEpoch 5 complete. Avg Train Loss: 2.6305\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:25<00:00,  6.95batch/s, avg_loss=2.54, batch_loss=2.13]\n","output_type":"stream"},{"name":"stdout","text":"Saving Checkpoint\nEpoch 6 complete. Avg Train Loss: 2.5448\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:25<00:00,  6.94batch/s, avg_loss=2.48, batch_loss=2.65]\n","output_type":"stream"},{"name":"stdout","text":"Saving Checkpoint\nEpoch 7 complete. Avg Train Loss: 2.4756\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:28<00:00,  6.82batch/s, avg_loss=2.42, batch_loss=2.82]\n","output_type":"stream"},{"name":"stdout","text":"Saving Checkpoint\nEpoch 8 complete. Avg Train Loss: 2.4167\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:25<00:00,  6.96batch/s, avg_loss=2.37, batch_loss=2.41]\n","output_type":"stream"},{"name":"stdout","text":"Saving Checkpoint\nEpoch 9 complete. Avg Train Loss: 2.3658\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [02:24<00:00,  7.00batch/s, avg_loss=2.32, batch_loss=2.58]\n","output_type":"stream"},{"name":"stdout","text":"Saving Checkpoint\nEpoch 10 complete. Avg Train Loss: 2.3212\nModel saved successfully!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!tensorboard --logdir=runs #for training details after completion","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T17:37:21.904242Z","iopub.execute_input":"2025-10-02T17:37:21.904877Z","iopub.status.idle":"2025-10-02T17:39:12.979186Z","shell.execute_reply.started":"2025-10-02T17:37:21.904846Z","shell.execute_reply":"2025-10-02T17:39:12.978469Z"}},"outputs":[{"name":"stdout","text":"2025-10-02 17:37:23.043647: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759426643.064453     237 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759426643.071423     237 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\n^C\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def eval():\n    model = CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers).to(device)\n    model.load_state_dict(torch.load(\"final_model.pth\"))\n    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n\n    model.eval()\n    total_val_loss = 0\n    loop = tqdm(\n        enumerate(val_loader),\n        total=len(val_loader),\n        desc=f\"Evaluation\",\n        unit=\"batch\",\n        leave=True\n    )\n    with torch.no_grad():\n        for batch_idx, (imgs, captions) in loop:\n            imgs, captions = imgs.to(device), captions.to(device)\n            outputs = model(imgs, captions[:-1])\n            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n            total_val_loss += loss.item()\n            loop.set_postfix(batch_loss=loss.item(),\n            avg_loss=total_val_loss/(batch_idx+1))\n    avg_val_loss = total_val_loss / len(val_loader)\n    print(f\"Evaluation complete. Avg Validation Loss: {avg_val_loss:.4f}\")\n\neval()","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:39:17.788714Z","iopub.execute_input":"2025-10-02T17:39:17.789009Z","iopub.status.idle":"2025-10-02T17:39:55.865233Z","shell.execute_reply.started":"2025-10-02T17:39:17.788984Z","shell.execute_reply":"2025-10-02T17:39:55.864413Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [00:37<00:00,  6.77batch/s, avg_loss=2.45, batch_loss=2.35]","output_type":"stream"},{"name":"stdout","text":"Evaluation complete. Avg Validation Loss: 2.4506\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def inference():\n    inference_transform = transforms.Compose(\n        [\n            transforms.Resize(356),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n        ]\n    )\n    \n    _,val_loader,dataset = get_loader(\n        root_folder=\"/kaggle/input/flickr8k/Images/\",\n        annotation_file=\"/kaggle/input/flickr8k/captions.txt\",\n        transform = inference_transform,\n        num_workers=2)\n    \n    model = CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers).to(device)\n    model.load_state_dict(torch.load(\"final_model.pth\"))\n    model.eval()\n    imgs, captions = next(iter(val_loader))\n    imgs, captions = imgs.to(device), captions.to(device)\n    y_pred = model.caption_image(imgs[3].unsqueeze(0), dataset.vocab)  # one image\n    print(\"\\nPredicted:\", \" \".join(y_pred))\n\n\ninference()\n    ","metadata":{"execution":{"iopub.status.busy":"2025-10-02T17:44:53.336899Z","iopub.execute_input":"2025-10-02T17:44:53.337716Z","iopub.status.idle":"2025-10-02T17:44:55.222320Z","shell.execute_reply.started":"2025-10-02T17:44:53.337686Z","shell.execute_reply":"2025-10-02T17:44:55.221325Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nPredicted: <SOS> a white dog is running through the grass . <EOS>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}